{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: textblob in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: click in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy textblob -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger: Package\n",
      "[nltk_data]     'average_perceptron_tagger' not found in index\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # tokenization\n",
    "nltk.download('stopwords') # stopwords removal\n",
    "nltk.download('average_perceptron_tagger') # POS tagging\n",
    "nltk.download('wordnet') # wordnet database and lemmatization\n",
    "nltk.download('omw-1.4') # streaming\n",
    "nltk.download('indian') # Indian language POS tagging\n",
    "nltk.download('maxent_ne_chunker') # chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'They told that their ages are 25, 27 and 31 respectively'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25', '27', '31']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = re.findall(r'\\d+', sent)\n",
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ages = [int(age) for age in ages]\n",
    "leng = len(ages)\n",
    "leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.66666666666667"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg = 0\n",
    "for age in ages:\n",
    "    avg += int(age)/leng\n",
    "avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = []\n",
    "for word in sent.split():\n",
    "    if word.isdigit():\n",
    "        ages.append(int(word))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()]) # This will consider '25,' as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white spaces : space, tab, new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to 23 Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " '23',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '?', '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_punc = [word for word in lst if not word.isalnum()]\n",
    "non_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.076923076923077"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(non_punc))/len(lst) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128077"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('ðŸ‘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤³à¥€\n"
     ]
    }
   ],
   "source": [
    "char = '\\u0933\\u0940'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2355"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('à¤³')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤³'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(2355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('à¤³')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'âš•'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(9877)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **âš•** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤—à¤£à¤ªà¤¤à¥€ à¤¬à¤¾à¤ªà¥à¤ªà¤¾ à¤®à¥‹à¤°à¤¯à¤¾ !! à¤®à¤‚à¤—à¤² à¤®à¥‚à¤°à¥à¤¤à¥€ à¤®à¥‹à¤°à¤¯à¤¾ !!'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = 'à¤—à¤£à¤ªà¤¤à¥€ à¤¬à¤¾à¤ªà¥à¤ªà¤¾ à¤®à¥‹à¤°à¤¯à¤¾ !! à¤®à¤‚à¤—à¤² à¤®à¥‚à¤°à¥à¤¤à¥€ à¤®à¥‹à¤°à¤¯à¤¾ !!'\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤—à¤£à¤ªà¤¤à¥€', 'à¤¬à¤¾à¤ªà¥à¤ªà¤¾', 'à¤®à¥‹à¤°à¤¯à¤¾', '!!', 'à¤®à¤‚à¤—à¤²', 'à¤®à¥‚à¤°à¥à¤¤à¥€', 'à¤®à¥‹à¤°à¤¯à¤¾', '!!']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤—à¤£à¤ªà¤¤à¤¿ à¤¬à¤¾à¤ªà¥à¤ªà¤¾ à¤®à¥‹à¤°à¤¯à¤¾ !! à¤®à¤‚à¤—à¤² à¤®à¥‚à¤°à¥à¤¤à¥€ à¤®à¥‹à¤°à¤¯à¤¾ !!'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.replace('à¤¤à¥€', 'à¤¤à¤¿', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤—à¤£à¤ªà¤¤à¥€ à¤¬à¤¾à¤ªà¥à¤ªà¤¾ à¤®à¥‹à¤°à¤¯à¤¾ !! à¤®à¤‚à¤—à¤² à¤®à¥‚à¤°à¥à¤¤à¥€ à¤®à¥‹à¤°à¤¯à¤¾ !!'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤¯à¥‹à¤—à¥‡à¤¶', 'à¤•à¥à¤£à¤¾à¤²', 'à¤•à¥à¤£à¤¾à¤²', 'à¤®à¤¾à¤¨à¤¸à¥€', 'à¤®à¤¾à¤¨à¤¸', 'à¤†à¤¦à¤¿à¤¤à¥à¤¯', 'à¤ªà¤¾à¤°à¥à¤¥']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['à¤¯à¥‹à¤—à¥‡à¤¶', 'à¤•à¥à¤£à¤¾à¤²' , 'à¤•à¥à¤£à¤¾à¤²' , 'à¤®à¤¾à¤¨à¤¸à¥€' , 'à¤®à¤¾à¤¨à¤¸' , 'à¤†à¤¦à¤¿à¤¤à¥à¤¯' , 'à¤ªà¤¾à¤°à¥à¤¥' ]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤®à¤¾à¤¨à¤¸à¥€\n",
      "à¤®à¤¾à¤¨à¤¸\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    if name.startswith('à¤®'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤®à¤¾à¤¨à¤¸\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    if name.endswith('à¤¸'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤°à¤¾à¤œà¤—à¤¡ à¤¹à¤¾ à¤­à¤¾à¤°à¤¤à¤¾à¤šà¥à¤¯à¤¾ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯à¤¾à¤¤à¥€à¤² à¤à¤• à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡. à¤°à¤¾à¤œà¤—à¤¡ Archived 2020-08-05 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤µà¤° à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¸à¥à¤µà¤°à¤¾à¤œà¥à¤¯à¤¾à¤šà¥€ à¤ªà¤¹à¤¿à¤²à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¹à¥‹à¤¤à¥€. à¤ªà¥à¤£à¥‡ à¤¶à¤¹à¤°à¤¾à¤šà¥à¤¯à¤¾ à¤¨à¥ˆà¤‹à¤¤à¥à¤¯à¥‡à¤²à¤¾. à¥ªà¥® à¤•à¤¿. à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤†à¤£à¤¿ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤² à¤µà¥‡à¤²à¥à¤¹à¥‡ à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤ à¤µ à¤­à¥‹à¤° à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾ à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾ à¥¨à¥ª à¤•à¤¿.à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€ à¤†à¤£à¤¿ à¤—à¥à¤‚à¤œà¤µà¤£à¥€ à¤¯à¤¾ à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤ à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾ à¤¡à¥‹à¤‚à¤—à¤° à¤‰à¤­à¤¾ à¤†à¤¹à¥‡. à¤®à¤¾à¤µà¤³ à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡ à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¤¾à¤§à¥à¤¯ à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤°à¤¾à¤œà¤—à¤¡ à¤†à¤£à¤¿ à¤¤à¥‹à¤°à¤£à¤¾ à¤¹à¥‡ à¤¦à¥‹à¤¨à¥à¤¹à¥€ à¤•à¤¿à¤²à¥à¤²à¥‡ à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤¹à¥‹à¤¤à¥‡. à¤¤à¥‹à¤°à¤£à¤¾ Archived 2020-09-20 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡ à¤²à¤¹à¤¾à¤¨ à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¹à¤¾ à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¸à¥‹à¤¯à¥€à¤šà¤¾ à¤¨à¤µà¥à¤¹à¤¤à¤¾. à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡ à¤°à¤¾à¤œà¤—à¤¡ à¤¦à¥à¤°à¥à¤—à¤® à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¬à¤°à¤¾à¤š à¤®à¥‹à¤ à¤¾ à¤†à¤¹à¥‡. à¤¶à¤¿à¤µà¤¾à¤¯ à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡ à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€ à¤¬à¤¾à¤œà¥‚à¤¨à¥‡ à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾ à¤à¤–à¤¾à¤¦à¥€ à¤Ÿà¥‡à¤•à¤¡à¥€ à¤•à¤¿à¤‚à¤µà¤¾ à¤¨à¤¦à¥€ à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š à¤²à¤¾à¤—à¤¤à¥‡. à¤à¤µà¤¢à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€,à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤†à¤ªà¤²à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€ Archived 2020-03-18 at the Wayback Machine. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€ à¤¨à¤¿à¤µà¤¡ à¤•à¥‡à¤²à¥€. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾ à¤¤à¥€à¤¨ à¤®à¤¾à¤šà¥à¤¯à¤¾ à¤µ à¤à¤• à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡. à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤–à¥‚à¤ª à¤‰à¤‚à¤š à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¥€ à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¥§à¥©à¥¯à¥ª à¤®à¥€à¤Ÿà¤° à¤†à¤¹à¥‡. à¤¦à¥à¤°à¥à¤—à¤°à¤¾à¤œ à¤°à¤¾à¤œà¤—à¤¡ à¤¤à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤¾à¤•à¤¾à¤‚à¤•à¥à¤·à¥‡à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¤¦à¤¾à¤–à¤µà¤¤à¥‹, à¤¤à¤° à¤•à¤¿à¤²à¥à¤²à¥‡ à¤°à¤¾à¤¯à¤—à¤¡ à¤¹à¤¾ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤•à¤°à¥à¤¤à¥ƒà¤¤à¥à¤µà¤¾à¤šà¤¾ à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¦à¤¾à¤–à¤µà¤¤à¥‹. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥à¤¯à¤¾ à¤®à¤§à¥à¤¯à¤µà¤°à¥à¤¤à¥€ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤‰à¤‚à¤š à¤¡à¥‹à¤‚à¤—à¤° à¤¤à¤¾à¤¸à¥‚à¤¨ à¤¤à¤¯à¤¾à¤° à¤•à¥‡à¤²à¥‡à¤²à¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤®à¥à¤¹à¤£à¤œà¥‡ à¤ªà¥ƒà¤¥à¥à¤µà¥€à¤¨à¥‡ à¤¸à¥à¤µà¤°à¥à¤—à¤¾à¤µà¤° à¤•à¥‡à¤²à¥‡à¤²à¥€ à¤¸à¥à¤µà¤¾à¤°à¥€ à¤¹à¥‹à¤¯.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtext = '''à¤°à¤¾à¤œà¤—à¤¡ à¤¹à¤¾ à¤­à¤¾à¤°à¤¤à¤¾à¤šà¥à¤¯à¤¾ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯à¤¾à¤¤à¥€à¤² à¤à¤• à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡. à¤°à¤¾à¤œà¤—à¤¡ Archived 2020-08-05 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤µà¤° à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¸à¥à¤µà¤°à¤¾à¤œà¥à¤¯à¤¾à¤šà¥€ à¤ªà¤¹à¤¿à¤²à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¹à¥‹à¤¤à¥€. à¤ªà¥à¤£à¥‡ à¤¶à¤¹à¤°à¤¾à¤šà¥à¤¯à¤¾ à¤¨à¥ˆà¤‹à¤¤à¥à¤¯à¥‡à¤²à¤¾. à¥ªà¥® à¤•à¤¿. à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤†à¤£à¤¿ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤² à¤µà¥‡à¤²à¥à¤¹à¥‡ à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤ à¤µ à¤­à¥‹à¤° à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾ à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾ à¥¨à¥ª à¤•à¤¿.à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€ à¤†à¤£à¤¿ à¤—à¥à¤‚à¤œà¤µà¤£à¥€ à¤¯à¤¾ à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤ à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾ à¤¡à¥‹à¤‚à¤—à¤° à¤‰à¤­à¤¾ à¤†à¤¹à¥‡. à¤®à¤¾à¤µà¤³ à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡ à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¤¾à¤§à¥à¤¯ à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤°à¤¾à¤œà¤—à¤¡ à¤†à¤£à¤¿ à¤¤à¥‹à¤°à¤£à¤¾ à¤¹à¥‡ à¤¦à¥‹à¤¨à¥à¤¹à¥€ à¤•à¤¿à¤²à¥à¤²à¥‡ à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤¹à¥‹à¤¤à¥‡. à¤¤à¥‹à¤°à¤£à¤¾ Archived 2020-09-20 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡ à¤²à¤¹à¤¾à¤¨ à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¹à¤¾ à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¸à¥‹à¤¯à¥€à¤šà¤¾ à¤¨à¤µà¥à¤¹à¤¤à¤¾. à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡ à¤°à¤¾à¤œà¤—à¤¡ à¤¦à¥à¤°à¥à¤—à¤® à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¬à¤°à¤¾à¤š à¤®à¥‹à¤ à¤¾ à¤†à¤¹à¥‡. à¤¶à¤¿à¤µà¤¾à¤¯ à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡ à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€ à¤¬à¤¾à¤œà¥‚à¤¨à¥‡ à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾ à¤à¤–à¤¾à¤¦à¥€ à¤Ÿà¥‡à¤•à¤¡à¥€ à¤•à¤¿à¤‚à¤µà¤¾ à¤¨à¤¦à¥€ à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š à¤²à¤¾à¤—à¤¤à¥‡. à¤à¤µà¤¢à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€,à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤†à¤ªà¤²à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€ Archived 2020-03-18 at the Wayback Machine. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€ à¤¨à¤¿à¤µà¤¡ à¤•à¥‡à¤²à¥€. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾ à¤¤à¥€à¤¨ à¤®à¤¾à¤šà¥à¤¯à¤¾ à¤µ à¤à¤• à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡. à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤–à¥‚à¤ª à¤‰à¤‚à¤š à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¥€ à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¥§à¥©à¥¯à¥ª à¤®à¥€à¤Ÿà¤° à¤†à¤¹à¥‡. à¤¦à¥à¤°à¥à¤—à¤°à¤¾à¤œ à¤°à¤¾à¤œà¤—à¤¡ à¤¤à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤¾à¤•à¤¾à¤‚à¤•à¥à¤·à¥‡à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¤¦à¤¾à¤–à¤µà¤¤à¥‹, à¤¤à¤° à¤•à¤¿à¤²à¥à¤²à¥‡ à¤°à¤¾à¤¯à¤—à¤¡ à¤¹à¤¾ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤•à¤°à¥à¤¤à¥ƒà¤¤à¥à¤µà¤¾à¤šà¤¾ à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¦à¤¾à¤–à¤µà¤¤à¥‹. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥à¤¯à¤¾ à¤®à¤§à¥à¤¯à¤µà¤°à¥à¤¤à¥€ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤‰à¤‚à¤š à¤¡à¥‹à¤‚à¤—à¤° à¤¤à¤¾à¤¸à¥‚à¤¨ à¤¤à¤¯à¤¾à¤° à¤•à¥‡à¤²à¥‡à¤²à¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤®à¥à¤¹à¤£à¤œà¥‡ à¤ªà¥ƒà¤¥à¥à¤µà¥€à¤¨à¥‡ à¤¸à¥à¤µà¤°à¥à¤—à¤¾à¤µà¤° à¤•à¥‡à¤²à¥‡à¤²à¥€ à¤¸à¥à¤µà¤¾à¤°à¥€ à¤¹à¥‹à¤¯.'''\n",
    "\n",
    "mtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤¹à¤¾',\n",
       " 'à¤­à¤¾à¤°à¤¤à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°',\n",
       " 'à¤°à¤¾à¤œà¥à¤¯à¤¾à¤¤à¥€à¤²',\n",
       " 'à¤à¤•',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'Archived',\n",
       " '2020-08-05',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Wayback',\n",
       " 'Machine',\n",
       " '.',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤µà¤°',\n",
       " 'à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œ',\n",
       " 'à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤¸à¥à¤µà¤°à¤¾à¤œà¥à¤¯à¤¾à¤šà¥€',\n",
       " 'à¤ªà¤¹à¤¿à¤²à¥€',\n",
       " 'à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€',\n",
       " 'à¤¹à¥‹à¤¤à¥€',\n",
       " '.',\n",
       " 'à¤ªà¥à¤£à¥‡',\n",
       " 'à¤¶à¤¹à¤°à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤¨à¥ˆà¤‹à¤¤à¥à¤¯à¥‡à¤²à¤¾',\n",
       " '.',\n",
       " 'à¥ªà¥®',\n",
       " 'à¤•à¤¿',\n",
       " '.',\n",
       " 'à¤®à¥€',\n",
       " '.',\n",
       " 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤°',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤ªà¥à¤£à¥‡',\n",
       " 'à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤²',\n",
       " 'à¤µà¥‡à¤²à¥à¤¹à¥‡',\n",
       " 'à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤',\n",
       " 'à¤µ',\n",
       " 'à¤­à¥‹à¤°',\n",
       " 'à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾',\n",
       " 'à¥¨à¥ª',\n",
       " 'à¤•à¤¿.à¤®à¥€',\n",
       " '.',\n",
       " 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤°',\n",
       " 'à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤—à¥à¤‚à¤œà¤µà¤£à¥€',\n",
       " 'à¤¯à¤¾',\n",
       " 'à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤',\n",
       " 'à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾',\n",
       " 'à¤¡à¥‹à¤‚à¤—à¤°',\n",
       " 'à¤‰à¤­à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤®à¤¾à¤µà¤³',\n",
       " 'à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡',\n",
       " 'à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤°',\n",
       " 'à¤¸à¤¾à¤§à¥à¤¯',\n",
       " 'à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤¤à¥‹à¤°à¤£à¤¾',\n",
       " 'à¤¹à¥‡',\n",
       " 'à¤¦à¥‹à¤¨à¥à¤¹à¥€',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥‡',\n",
       " 'à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤ à¤¿à¤•à¤¾à¤£à¥€',\n",
       " 'à¤¹à¥‹à¤¤à¥‡',\n",
       " '.',\n",
       " 'à¤¤à¥‹à¤°à¤£à¤¾',\n",
       " 'Archived',\n",
       " '2020-09-20',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Wayback',\n",
       " 'Machine',\n",
       " '.',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡',\n",
       " 'à¤²à¤¹à¤¾à¤¨',\n",
       " 'à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤•à¥€à¤¯',\n",
       " 'à¤•à¥‡à¤‚à¤¦à¥à¤°',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤¹à¤¾',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤¸à¥‹à¤¯à¥€à¤šà¤¾',\n",
       " 'à¤¨à¤µà¥à¤¹à¤¤à¤¾',\n",
       " '.',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤¦à¥à¤°à¥à¤—à¤®',\n",
       " 'à¤…à¤¸à¥‚à¤¨',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤¬à¤°à¤¾à¤š',\n",
       " 'à¤®à¥‹à¤ à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤¯',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡',\n",
       " 'à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€',\n",
       " 'à¤¬à¤¾à¤œà¥‚à¤¨à¥‡',\n",
       " 'à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾',\n",
       " 'à¤à¤–à¤¾à¤¦à¥€',\n",
       " 'à¤Ÿà¥‡à¤•à¤¡à¥€',\n",
       " 'à¤•à¤¿à¤‚à¤µà¤¾',\n",
       " 'à¤¨à¤¦à¥€',\n",
       " 'à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š',\n",
       " 'à¤²à¤¾à¤—à¤¤à¥‡',\n",
       " '.',\n",
       " 'à¤à¤µà¤¢à¥€',\n",
       " 'à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾',\n",
       " 'à¤¹à¥‹à¤¤à¥€',\n",
       " ',',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤†à¤ªà¤²à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤•à¥€à¤¯',\n",
       " 'à¤•à¥‡à¤‚à¤¦à¥à¤°',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€',\n",
       " 'Archived',\n",
       " '2020-03-18',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Wayback',\n",
       " 'Machine',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€',\n",
       " 'à¤¨à¤¿à¤µà¤¡',\n",
       " 'à¤•à¥‡à¤²à¥€',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾',\n",
       " 'à¤¤à¥€à¤¨',\n",
       " 'à¤®à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤µ',\n",
       " 'à¤à¤•',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤–à¥‚à¤ª',\n",
       " 'à¤‰à¤‚à¤š',\n",
       " 'à¤…à¤¸à¥‚à¤¨',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤šà¥€',\n",
       " 'à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€',\n",
       " 'à¤‰à¤‚à¤šà¥€',\n",
       " 'à¥§à¥©à¥¯à¥ª',\n",
       " 'à¤®à¥€à¤Ÿà¤°',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤¦à¥à¤°à¥à¤—à¤°à¤¾à¤œ',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤¾à¤•à¤¾à¤‚à¤•à¥à¤·à¥‡à¤šà¥€',\n",
       " 'à¤‰à¤‚à¤šà¥€',\n",
       " 'à¤¦à¤¾à¤–à¤µà¤¤à¥‹',\n",
       " ',',\n",
       " 'à¤¤à¤°',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥‡',\n",
       " 'à¤°à¤¾à¤¯à¤—à¤¡',\n",
       " 'à¤¹à¤¾',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤•à¤°à¥à¤¤à¥ƒà¤¤à¥à¤µà¤¾à¤šà¤¾',\n",
       " 'à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤°',\n",
       " 'à¤¦à¤¾à¤–à¤µà¤¤à¥‹',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤®à¤§à¥à¤¯à¤µà¤°à¥à¤¤à¥€',\n",
       " 'à¤ à¤¿à¤•à¤¾à¤£à¥€',\n",
       " 'à¤‰à¤‚à¤š',\n",
       " 'à¤¡à¥‹à¤‚à¤—à¤°',\n",
       " 'à¤¤à¤¾à¤¸à¥‚à¤¨',\n",
       " 'à¤¤à¤¯à¤¾à¤°',\n",
       " 'à¤•à¥‡à¤²à¥‡à¤²à¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤®à¥à¤¹à¤£à¤œà¥‡',\n",
       " 'à¤ªà¥ƒà¤¥à¥à¤µà¥€à¤¨à¥‡',\n",
       " 'à¤¸à¥à¤µà¤°à¥à¤—à¤¾à¤µà¤°',\n",
       " 'à¤•à¥‡à¤²à¥‡à¤²à¥€',\n",
       " 'à¤¸à¥à¤µà¤¾à¤°à¥€',\n",
       " 'à¤¹à¥‹à¤¯',\n",
       " '.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤°à¤¾à¤œà¤—à¤¡ à¤¹à¤¾ à¤­à¤¾à¤°à¤¤à¤¾à¤šà¥à¤¯à¤¾ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯à¤¾à¤¤à¥€à¤² à¤à¤• à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡ Archived 2020-08-05 at the Wayback Machine.',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤µà¤° à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¸à¥à¤µà¤°à¤¾à¤œà¥à¤¯à¤¾à¤šà¥€ à¤ªà¤¹à¤¿à¤²à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¹à¥‹à¤¤à¥€.',\n",
       " 'à¤ªà¥à¤£à¥‡ à¤¶à¤¹à¤°à¤¾à¤šà¥à¤¯à¤¾ à¤¨à¥ˆà¤‹à¤¤à¥à¤¯à¥‡à¤²à¤¾.',\n",
       " 'à¥ªà¥® à¤•à¤¿.',\n",
       " 'à¤®à¥€.',\n",
       " 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤†à¤£à¤¿ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤² à¤µà¥‡à¤²à¥à¤¹à¥‡ à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤ à¤µ à¤­à¥‹à¤° à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾ à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾ à¥¨à¥ª à¤•à¤¿.à¤®à¥€.',\n",
       " 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€ à¤†à¤£à¤¿ à¤—à¥à¤‚à¤œà¤µà¤£à¥€ à¤¯à¤¾ à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤ à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾ à¤¡à¥‹à¤‚à¤—à¤° à¤‰à¤­à¤¾ à¤†à¤¹à¥‡.',\n",
       " 'à¤®à¤¾à¤µà¤³ à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡ à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¤¾à¤§à¥à¤¯ à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤°à¤¾à¤œà¤—à¤¡ à¤†à¤£à¤¿ à¤¤à¥‹à¤°à¤£à¤¾ à¤¹à¥‡ à¤¦à¥‹à¤¨à¥à¤¹à¥€ à¤•à¤¿à¤²à¥à¤²à¥‡ à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤¹à¥‹à¤¤à¥‡.',\n",
       " 'à¤¤à¥‹à¤°à¤£à¤¾ Archived 2020-09-20 at the Wayback Machine.',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡ à¤²à¤¹à¤¾à¤¨ à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¹à¤¾ à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¸à¥‹à¤¯à¥€à¤šà¤¾ à¤¨à¤µà¥à¤¹à¤¤à¤¾.',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡ à¤°à¤¾à¤œà¤—à¤¡ à¤¦à¥à¤°à¥à¤—à¤® à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¬à¤°à¤¾à¤š à¤®à¥‹à¤ à¤¾ à¤†à¤¹à¥‡.',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤¯ à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡ à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€ à¤¬à¤¾à¤œà¥‚à¤¨à¥‡ à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾ à¤à¤–à¤¾à¤¦à¥€ à¤Ÿà¥‡à¤•à¤¡à¥€ à¤•à¤¿à¤‚à¤µà¤¾ à¤¨à¤¦à¥€ à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š à¤²à¤¾à¤—à¤¤à¥‡.',\n",
       " 'à¤à¤µà¤¢à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€,à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤†à¤ªà¤²à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€ Archived 2020-03-18 at the Wayback Machine.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€ à¤¨à¤¿à¤µà¤¡ à¤•à¥‡à¤²à¥€.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾ à¤¤à¥€à¤¨ à¤®à¤¾à¤šà¥à¤¯à¤¾ à¤µ à¤à¤• à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤–à¥‚à¤ª à¤‰à¤‚à¤š à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¥€ à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¥§à¥©à¥¯à¥ª à¤®à¥€à¤Ÿà¤° à¤†à¤¹à¥‡.',\n",
       " 'à¤¦à¥à¤°à¥à¤—à¤°à¤¾à¤œ à¤°à¤¾à¤œà¤—à¤¡ à¤¤à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤¾à¤•à¤¾à¤‚à¤•à¥à¤·à¥‡à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¤¦à¤¾à¤–à¤µà¤¤à¥‹, à¤¤à¤° à¤•à¤¿à¤²à¥à¤²à¥‡ à¤°à¤¾à¤¯à¤—à¤¡ à¤¹à¤¾ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤•à¤°à¥à¤¤à¥ƒà¤¤à¥à¤µà¤¾à¤šà¤¾ à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¦à¤¾à¤–à¤µà¤¤à¥‹.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥à¤¯à¤¾ à¤®à¤§à¥à¤¯à¤µà¤°à¥à¤¤à¥€ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤‰à¤‚à¤š à¤¡à¥‹à¤‚à¤—à¤° à¤¤à¤¾à¤¸à¥‚à¤¨ à¤¤à¤¯à¤¾à¤° à¤•à¥‡à¤²à¥‡à¤²à¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤®à¥à¤¹à¤£à¤œà¥‡ à¤ªà¥ƒà¤¥à¥à¤µà¥€à¤¨à¥‡ à¤¸à¥à¤µà¤°à¥à¤—à¤¾à¤µà¤° à¤•à¥‡à¤²à¥‡à¤²à¥€ à¤¸à¥à¤µà¤¾à¤°à¥€ à¤¹à¥‹à¤¯.']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(mtext) # sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello friends!  How are you? \n",
      "Welcome to the world of python programming.\n",
      "\n",
      " appended text\n",
      " appended text\n",
      " appended text\n"
     ]
    }
   ],
   "source": [
    "f = open(\"test.txt\", \"a\")\n",
    "f.write(\"\\n appended text\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"test.txt\", \"r\")\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can't hv non-readable text file in windows, we can have it in Linux and MacOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Hello friends! How are you? \n",
      "\n",
      "Welcome to the world of python programming.\n",
      "\n",
      "-------\n",
      "Hello friends! How are you? \n",
      "\n",
      "Welcome to the world of python programming.\n",
      "\n",
      " appended text\n",
      "\n",
      " appended text\n",
      "\n",
      " appended text\n",
      "-------\n",
      "Hello friends! How are you? \n",
      "Welcome to the world of python programming.\n",
      " appended text\n",
      " appended text\n",
      " appended text\n"
     ]
    }
   ],
   "source": [
    "print(f.readable())\n",
    "\n",
    "with open('test.txt', 'r') as f:\n",
    "    line1 = f.readline()\n",
    "    line2 = f.readline()\n",
    "    print(line1)  # Print the first line\n",
    "    print(line2)  # Print the second line\n",
    "\n",
    "print('-------')\n",
    "\n",
    "with open('test.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "\n",
    "print('-------')\n",
    "\n",
    "f = open(\"test.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello friends!\\tHow are you? \\nWelcome to the world of\\tpython programming.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('test.txt')\n",
    "data = f.read()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!\\tHow',\n",
       " 'are',\n",
       " 'you?',\n",
       " '\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of\\tpython',\n",
       " 'programming.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "# create the object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tab Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!',\n",
       " 'How are you? \\nWelcome to the world of',\n",
       " 'python programming.']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "# create the object\n",
    "tk = TabTokenizer()\n",
    "\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!\\tHow are you? ',\n",
       " 'Welcome to the world of\\tpython programming.']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "# create the object\n",
    "tk = LineTokenizer()\n",
    "\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitespace Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# create the object\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MWE Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '''The Van Rossum :)is Python creator, visiting Pune this week. The\n",
    "development community is very eager to meet :) Van Rossum.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MWETokenizer in module nltk.tokenize.mwe:\n",
      "\n",
      "class MWETokenizer(nltk.tokenize.api.TokenizerI)\n",
      " |  MWETokenizer(mwes=None, separator='_')\n",
      " |  \n",
      " |  A tokenizer that processes tokenized text and merges multi-word expressions\n",
      " |  into single tokens.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MWETokenizer\n",
      " |      nltk.tokenize.api.TokenizerI\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, mwes=None, separator='_')\n",
      " |      Initialize the multi-word tokenizer with a list of expressions and a\n",
      " |      separator\n",
      " |      \n",
      " |      :type mwes: list(list(str))\n",
      " |      :param mwes: A sequence of multi-word expressions to be merged, where\n",
      " |          each MWE is a sequence of strings.\n",
      " |      :type separator: str\n",
      " |      :param separator: String that should be inserted between words in a multi-word\n",
      " |          expression token. (Default is '_')\n",
      " |  \n",
      " |  add_mwe(self, mwe)\n",
      " |      Add a multi-word expression to the lexicon (stored as a word trie)\n",
      " |      \n",
      " |      We use ``util.Trie`` to represent the trie. Its form is a dict of dicts.\n",
      " |      The key True marks the end of a valid MWE.\n",
      " |      \n",
      " |      :param mwe: The multi-word expression we're adding into the word trie\n",
      " |      :type mwe: tuple(str) or list(str)\n",
      " |      \n",
      " |      :Example:\n",
      " |      \n",
      " |      >>> tokenizer = MWETokenizer()\n",
      " |      >>> tokenizer.add_mwe(('a', 'b'))\n",
      " |      >>> tokenizer.add_mwe(('a', 'b', 'c'))\n",
      " |      >>> tokenizer.add_mwe(('a', 'x'))\n",
      " |      >>> expected = {'a': {'x': {True: None}, 'b': {True: None, 'c': {True: None}}}}\n",
      " |      >>> tokenizer._mwes == expected\n",
      " |      True\n",
      " |  \n",
      " |  tokenize(self, text)\n",
      " |      :param text: A list containing tokenized text\n",
      " |      :type text: list(str)\n",
      " |      :return: A list of the tokenized text with multi-words merged together\n",
      " |      :rtype: list(str)\n",
      " |      \n",
      " |      :Example:\n",
      " |      \n",
      " |      >>> tokenizer = MWETokenizer([('hors', \"d'oeuvre\")], separator='+')\n",
      " |      >>> tokenizer.tokenize(\"An hors d'oeuvre tonight, sir?\".split())\n",
      " |      ['An', \"hors+d'oeuvre\", 'tonight,', 'sir?']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tokenize.api.TokenizerI:\n",
      " |  \n",
      " |  span_tokenize(self, s: str) -> Iterator[Tuple[int, int]]\n",
      " |      Identify the tokens using integer offsets ``(start_i, end_i)``,\n",
      " |      where ``s[start_i:end_i]`` is the corresponding token.\n",
      " |      \n",
      " |      :rtype: Iterator[Tuple[int, int]]\n",
      " |  \n",
      " |  span_tokenize_sents(self, strings: List[str]) -> Iterator[List[Tuple[int, int]]]\n",
      " |      Apply ``self.span_tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.span_tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :yield: List[Tuple[int, int]]\n",
      " |  \n",
      " |  tokenize_sents(self, strings: List[str]) -> List[List[str]]\n",
      " |      Apply ``self.tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :rtype: List[List[str]]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.tokenize.api.TokenizerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MWETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m \u001b[0mMWETokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmwes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "A tokenizer that processes tokenized text and merges multi-word expressions\n",
      "into single tokens.\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Initialize the multi-word tokenizer with a list of expressions and a\n",
      "separator\n",
      "\n",
      ":type mwes: list(list(str))\n",
      ":param mwes: A sequence of multi-word expressions to be merged, where\n",
      "    each MWE is a sequence of strings.\n",
      ":type separator: str\n",
      ":param separator: String that should be inserted between words in a multi-word\n",
      "    expression token. (Default is '_')\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\administrator.dai-pc2\\.conda\\envs\\new\\lib\\site-packages\\nltk\\tokenize\\mwe.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "MWETokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van:)Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van:)Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# create the object\n",
    "tk = MWETokenizer(separator=\":)\")\n",
    "\n",
    "# add multi word expression\n",
    "tk.add_mwe(('Van', 'Rossum'))\n",
    "\n",
    "tk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweet token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " ':)',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " ':)',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# create the object\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "\n",
    "tk.tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello friends!\\tHow are you? ðŸ˜Š\\nWelcome to the worldðŸ™Œ of\\tpython programming. '"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('test.txt', encoding=\"utf-8\")\n",
    "data = f.read()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ˜Š',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'ðŸ™Œ',\n",
       " 'of',\n",
       " 'python',\n",
       " 'programming',\n",
       " '.']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# create the object\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclosefd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mopener\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Open file and return a stream.  Raise OSError upon failure.\n",
      "\n",
      "file is either a text or byte string giving the name (and the path\n",
      "if the file isn't in the current working directory) of the file to\n",
      "be opened or an integer file descriptor of the file to be\n",
      "wrapped. (If a file descriptor is given, it is closed when the\n",
      "returned I/O object is closed, unless closefd is set to False.)\n",
      "\n",
      "mode is an optional string that specifies the mode in which the file\n",
      "is opened. It defaults to 'r' which means open for reading in text\n",
      "mode.  Other common values are 'w' for writing (truncating the file if\n",
      "it already exists), 'x' for creating and writing to a new file, and\n",
      "'a' for appending (which on some Unix systems, means that all writes\n",
      "append to the end of the file regardless of the current seek position).\n",
      "In text mode, if encoding is not specified the encoding used is platform\n",
      "dependent: locale.getpreferredencoding(False) is called to get the\n",
      "current locale encoding. (For reading and writing raw bytes use binary\n",
      "mode and leave encoding unspecified.) The available modes are:\n",
      "\n",
      "========= ===============================================================\n",
      "Character Meaning\n",
      "--------- ---------------------------------------------------------------\n",
      "'r'       open for reading (default)\n",
      "'w'       open for writing, truncating the file first\n",
      "'x'       create a new file and open it for writing\n",
      "'a'       open for writing, appending to the end of the file if it exists\n",
      "'b'       binary mode\n",
      "'t'       text mode (default)\n",
      "'+'       open a disk file for updating (reading and writing)\n",
      "'U'       universal newline mode (deprecated)\n",
      "========= ===============================================================\n",
      "\n",
      "The default mode is 'rt' (open for reading text). For binary random\n",
      "access, the mode 'w+b' opens and truncates the file to 0 bytes, while\n",
      "'r+b' opens the file without truncation. The 'x' mode implies 'w' and\n",
      "raises an `FileExistsError` if the file already exists.\n",
      "\n",
      "Python distinguishes between files opened in binary and text modes,\n",
      "even when the underlying operating system doesn't. Files opened in\n",
      "binary mode (appending 'b' to the mode argument) return contents as\n",
      "bytes objects without any decoding. In text mode (the default, or when\n",
      "'t' is appended to the mode argument), the contents of the file are\n",
      "returned as strings, the bytes having been first decoded using a\n",
      "platform-dependent encoding or using the specified encoding if given.\n",
      "\n",
      "'U' mode is deprecated and will raise an exception in future versions\n",
      "of Python.  It has no effect in Python 3.  Use newline to control\n",
      "universal newlines mode.\n",
      "\n",
      "buffering is an optional integer used to set the buffering policy.\n",
      "Pass 0 to switch buffering off (only allowed in binary mode), 1 to select\n",
      "line buffering (only usable in text mode), and an integer > 1 to indicate\n",
      "the size of a fixed-size chunk buffer.  When no buffering argument is\n",
      "given, the default buffering policy works as follows:\n",
      "\n",
      "* Binary files are buffered in fixed-size chunks; the size of the buffer\n",
      "  is chosen using a heuristic trying to determine the underlying device's\n",
      "  \"block size\" and falling back on `io.DEFAULT_BUFFER_SIZE`.\n",
      "  On many systems, the buffer will typically be 4096 or 8192 bytes long.\n",
      "\n",
      "* \"Interactive\" text files (files for which isatty() returns True)\n",
      "  use line buffering.  Other text files use the policy described above\n",
      "  for binary files.\n",
      "\n",
      "encoding is the name of the encoding used to decode or encode the\n",
      "file. This should only be used in text mode. The default encoding is\n",
      "platform dependent, but any encoding supported by Python can be\n",
      "passed.  See the codecs module for the list of supported encodings.\n",
      "\n",
      "errors is an optional string that specifies how encoding errors are to\n",
      "be handled---this argument should not be used in binary mode. Pass\n",
      "'strict' to raise a ValueError exception if there is an encoding error\n",
      "(the default of None has the same effect), or pass 'ignore' to ignore\n",
      "errors. (Note that ignoring encoding errors can lead to data loss.)\n",
      "See the documentation for codecs.register or run 'help(codecs.Codec)'\n",
      "for a list of the permitted encoding error strings.\n",
      "\n",
      "newline controls how universal newlines works (it only applies to text\n",
      "mode). It can be None, '', '\\n', '\\r', and '\\r\\n'.  It works as\n",
      "follows:\n",
      "\n",
      "* On input, if newline is None, universal newlines mode is\n",
      "  enabled. Lines in the input can end in '\\n', '\\r', or '\\r\\n', and\n",
      "  these are translated into '\\n' before being returned to the\n",
      "  caller. If it is '', universal newline mode is enabled, but line\n",
      "  endings are returned to the caller untranslated. If it has any of\n",
      "  the other legal values, input lines are only terminated by the given\n",
      "  string, and the line ending is returned to the caller untranslated.\n",
      "\n",
      "* On output, if newline is None, any '\\n' characters written are\n",
      "  translated to the system default line separator, os.linesep. If\n",
      "  newline is '' or '\\n', no translation takes place. If newline is any\n",
      "  of the other legal values, any '\\n' characters written are translated\n",
      "  to the given string.\n",
      "\n",
      "If closefd is False, the underlying file descriptor will be kept open\n",
      "when the file is closed. This does not work when a file name is given\n",
      "and must be True in that case.\n",
      "\n",
      "A custom opener can be used by passing a callable as *opener*. The\n",
      "underlying file descriptor for the file object is then obtained by\n",
      "calling *opener* with (*file*, *flags*). *opener* must return an open\n",
      "file descriptor (passing os.open as *opener* results in functionality\n",
      "similar to passing None).\n",
      "\n",
      "open() returns a file object whose type depends on the mode, and\n",
      "through which the standard file operations such as reading and writing\n",
      "are performed. When open() is used to open a file in a text mode ('w',\n",
      "'r', 'wt', 'rt', etc.), it returns a TextIOWrapper. When used to open\n",
      "a file in a binary mode, the returned class varies: in read binary\n",
      "mode, it returns a BufferedReader; in write binary and append binary\n",
      "modes, it returns a BufferedWriter, and in read/write mode, it returns\n",
      "a BufferedRandom.\n",
      "\n",
      "It is also possible to use a string or bytearray as a file for both\n",
      "reading and writing. For strings StringIO can be used like a file\n",
      "opened in a text mode, and for bytes a BytesIO can be used like a file\n",
      "opened in a binary mode.\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "open?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "This is some text with punctuation > Let's \n",
      " tokenize it\n",
      " Is it ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\$]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's $ tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens: \")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mitu.co.in/dataset\n",
    "# student3.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'anil', 'TE', '56.77', '22']\n",
      "['2', 'amit', 'TE', '59.77', '21']\n",
      "['3', 'aniket', 'BE', '76.88', '19']\n",
      "['4', 'ajinkya', 'TE', '69.66', '20']\n",
      "['5', 'asha', 'TE', '63.28', '20']\n",
      "['6', 'ayesha', 'BE', '49.55', '20']\n",
      "['7', 'amar', 'BE', '65.34', '19']\n",
      "['8', 'amita', 'BE', '68.33', '23']\n",
      "['9', 'amol', 'TE', '56.75', '20']\n",
      "['10', 'anmol', 'BE', '78.66', '21']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "split1 = data.split('\\n')\n",
    "for i in range(1, len(split1)):\n",
    "    split = split1[i].split('\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21]]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata = []\n",
    "split_n = data.split('\\n')\n",
    "for x in range(1, len(split_n)):\n",
    "    inner_list = []\n",
    "    for y in split_n[x].split('\\t'):\n",
    "        if y.isdigit():\n",
    "            inner_list.append(int(y))\n",
    "        elif y.find('.') > 0:\n",
    "            inner_list.append(float(y))\n",
    "        else:\n",
    "            inner_list.append(y)\n",
    "    if len(inner_list) == 1:\n",
    "        continue\n",
    "    newdata.append(inner_list)\n",
    "newdata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import Line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ajinkya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
